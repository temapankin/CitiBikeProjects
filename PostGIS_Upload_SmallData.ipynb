{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load packages\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the DATABASE\n",
    "# Change those values to match your database\n",
    "hostNameStr = 'localhost'; portNumber  = '5432'\n",
    "userNameStr = 'postgres' ; passwordStr = '1234'\n",
    "databaseStr = 'postgres' ; \n",
    "\n",
    "# the connection format is 'dialect+driver://username:password@host:port/database'\n",
    "dbEngine = create_engine('postgresql://' \\\n",
    "    + userNameStr + ':' + passwordStr \\\n",
    "        + '@' + hostNameStr + ':' + portNumber \\\n",
    "            + '/' + databaseStr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and write non-spatial data from/to PostgreSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small non-spatial dataset\n",
    "For small non-spatial dataset, we can directly read and write using pandas: read/to_csv/sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "858"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For small non-spatial dataset, we can directly read and write using pandas: read/to_csv/sql\n",
    "tableName = 'Blocks'\n",
    "csvFileName =  r'/Users/artyom/Downloads/Decennial DHC 2020/DECENNIALDHC2020.P1-Data.csv'\n",
    "\n",
    "# Read a csv file and directly write to a new table\n",
    "# The function will create all the columns automatically\n",
    "df = pd.read_csv(csvFileName)\n",
    "df.to_sql(tableName, dbEngine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large non-spatial dataset\n",
    "For large dataset, we need to break down the data into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "# This uses direct copy for insert, which is much faster than regular insert.\n",
    "# Do NOT change this function, unless you really know what it does\n",
    "def psql_insert_copy(table, conn, keys, data_iter):\n",
    "    \"\"\"\n",
    "    Execute SQL statement inserting data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    table : pandas.io.sql.SQLTable\n",
    "    conn : sqlalchemy.engine.Engine or sqlalchemy.engine.Connection\n",
    "    keys : list of str\n",
    "        Column names\n",
    "    data_iter : Iterable that iterates the values to be inserted\n",
    "    \"\"\"\n",
    "    # gets a DBAPI connection that can provide a cursor\n",
    "    dbapi_conn = conn.connection\n",
    "    with dbapi_conn.cursor() as cur:\n",
    "        s_buf = StringIO()\n",
    "        writer = csv.writer(s_buf)\n",
    "        writer.writerows(data_iter)\n",
    "        s_buf.seek(0)\n",
    "\n",
    "        columns = ', '.join('\"{}\"'.format(k) for k in keys)\n",
    "        if table.schema:\n",
    "            table_name = '{}.{}'.format(table.schema, table.name)\n",
    "        else:\n",
    "            table_name = table.name\n",
    "\n",
    "        sql = 'COPY {} ({}) FROM STDIN WITH CSV'.format(\n",
    "            table_name, columns)\n",
    "        cur.copy_expert(sql=sql, file=s_buf)\n",
    "\n",
    "\n",
    "# the csv file name\n",
    "csvFileName = r'nyc311.csv' # This is a 15GB or so file from NYC 311\n",
    "tableName = 'serequst'\n",
    "\n",
    "with pd.read_csv(csvFileName, sep=\",\", index_col = 0, chunksize=4096) as reader:\n",
    "    # reader\n",
    "    for chunk in reader:\n",
    "        chunk.columns = map(str.lower, chunk.columns)\n",
    "        chunk.columns = chunk.columns.str.replace(' ','_')\n",
    "        chunk.columns.str.replace('[\\W\\_]','')\n",
    "        chunk.columns = chunk.columns.str.replace('[^A-Za-z0-9_]+','')\n",
    "\n",
    "        # It may require forcing a column to a specific type. \n",
    "        # For example, zip codes commonly are numbers. \n",
    "        # But if some values contain \"-\", the column needs to be converted to text.\n",
    "        \n",
    "        chunk.to_sql(tableName, dbEngine, if_exists = 'append', method = psql_insert_copy)\n",
    "        #print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read/Write spatial data from/to PostgreSQL/PostGIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small spatial dataset\n",
    "For regular spatial dataset, we can use geopandas package to read and write between PostGIS and other formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from a Shapefile and write to the database, public schema\n",
    "nta_dat = gpd.read_file(\"./data/nyc_nta.shp\")\n",
    "nta_dat.to_postgis('nta_table', dbEngine, schema='public', if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve data from a PostGIS table. \n",
    "# If the geometry column is not named \"geom\", we have to specify its name.\n",
    "sql = 'SELECT objectid, bin, base_elevation, top_elevation, height, \"SHAPE\" FROM water_tank'\n",
    "df = gpd.read_postgis(sql, dbEngine, geom_col='SHAPE')  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "074dba7ff5f39c60f8d1e1c71fa9334245422e66417a9f1b182d2451b4a6699e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
